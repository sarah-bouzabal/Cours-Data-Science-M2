

## ğŸ“ **Subsection A â€“ Introduction to Machine Learning**

> **Lecture Duration**: \~2 hours
> **Slides**: \~25â€“30

---

### ğŸŸ¦ Slide 1 â€“ Title Slide

**Title**: Introduction to Machine Learning
**Subtitle**: Data Science Course â€“ M2 AI & Data Science
**Presented by**: *Ali Mokh*

> *Optional image: AI in the real world (self-driving car, fraud detection, etc.)*

---

### ğŸŸ¦ Slide 2 â€“ Learning Objectives

By the end of this lecture, you will be able to:

* Explain what machine learning is and how it differs from traditional programming
* Identify different types of machine learning (supervised vs. unsupervised)
* Understand the full ML pipeline: from data to deployment
* Recognize overfitting, underfitting, and generalization
* Use Scikit-learnâ€™s API and pipelines

---

### ğŸŸ¦ Slide 3 â€“ What is Machine Learning?

* A method of teaching computers to learn from data
* **Tom Mitchell's Definition**:

  > *â€œA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.â€*
* In contrast to **explicit rule-based programming**

---

### ğŸŸ¦ Slide 4 â€“ Traditional Programming vs. ML

| Traditional Programming | Machine Learning      |
| ----------------------- | --------------------- |
| Rules + Data â†’ Output   | Data + Output â†’ Rules |
| Hand-coded logic        | Model learns patterns |
| Static behavior         | Adaptive behavior     |

> *Speaker note: Give a spam filtering example â€“ rules vs. ML.*

---

### ğŸŸ¦ Slide 5 â€“ ML in the Real World

* Credit scoring
* Medical diagnosis
* Personalized recommendations
* Voice assistants
* Image recognition
* Predictive maintenance

---

### ğŸŸ¦ Slide 6 â€“ Categories of Machine Learning

* Supervised Learning
* Unsupervised Learning
* Semi-supervised Learning
* Reinforcement Learning

> We'll focus on the first two in this course.

---

### ğŸŸ¦ Slide 7 â€“ Supervised Learning

* Learn from **labeled data**
* Objective: predict output `y` from input `X`
* **Tasks**:

  * Regression â†’ continuous `y`
  * Classification â†’ discrete labels
* ğŸ“Œ Example: Predicting house prices from features like area, rooms

---

### ğŸŸ¦ Slide 8 â€“ Unsupervised Learning

* No labels, only input data `X`
* Discover patterns or structures
* **Tasks**:

  * Clustering (e.g., customer segmentation)
  * Dimensionality Reduction (e.g., PCA)
* ğŸ“Œ Example: Grouping customers by behavior without labels

---

### ğŸŸ¦ Slide 9 â€“ Visual: Supervised vs. Unsupervised

> Show a graphic:

* Labeled dataset â†’ model
* Unlabeled dataset â†’ clusters

---

### ğŸŸ¦ Slide 10 â€“ Machine Learning Workflow

Typical ML pipeline:

1. Data Collection
2. Data Cleaning
3. Feature Engineering
4. Model Selection
5. Training
6. Evaluation
7. Hyperparameter Tuning
8. Deployment & Monitoring

> *Each stage will be explored in more detail throughout the course.*

---

### ğŸŸ¦ Slide 11 â€“ Key Terminologies

* **Features**: Independent variables (X)
* **Target/Label**: Dependent variable (y)
* **Sample/Instance**: A single observation
* **Model**: Mathematical function mapping X to y
* **Loss Function**: Quantifies model error
* **Learning Algorithm**: Adjusts parameters to minimize loss

---

### ğŸŸ¦ Slide 12 â€“ Train/Test Split

* Separate dataset into:

  * **Training Set**: Used to train the model
  * **Test Set**: Used to evaluate generalization
* Typical split: 80/20 or 70/30
* Prevents **data leakage**

---

### ğŸŸ¦ Slide 13 â€“ Overfitting vs. Underfitting

* **Underfitting**:

  * Too simple
  * High bias
* **Overfitting**:

  * Too complex
  * High variance
* Ideal: low bias and variance

> *Include diagram: model complexity vs. error*

---

### ğŸŸ¦ Slide 14 â€“ Generalization & Cross-Validation

* Generalization: modelâ€™s performance on unseen data
* **k-Fold Cross-Validation**:

  * Data split into k parts
  * Each part used as validation once
  * Reduces variance in evaluation

---

### ğŸŸ¦ Slide 15 â€“ Biasâ€“Variance Tradeoff

* Total Error = BiasÂ² + Variance + Irreducible Error
* Need balance:

  * High bias â†’ underfit
  * High variance â†’ overfit

> *Include visualization with curves*

---

### ğŸŸ¦ Slide 16 â€“ Introduction to Scikit-learn

* Pythonâ€™s most widely used ML library
* Clean and consistent API
* Integrates:

  * Preprocessing
  * Modeling
  * Validation
  * Pipelines

---

### ğŸŸ¦ Slide 17 â€“ Estimator API

Every ML model in Scikit-learn follows this structure:

```python
model = ModelClass()
model.fit(X_train, y_train)
predictions = model.predict(X_test)
```

---

### ğŸŸ¦ Slide 18 â€“ Transformer & Preprocessing API

For preprocessing:

```python
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)
```

Other examples:

* `MinMaxScaler`
* `OneHotEncoder`
* `SimpleImputer`

---

### ğŸŸ¦ Slide 19 â€“ Scikit-learn Pipeline Example

Combine preprocessing + model:

```python
from sklearn.pipeline import Pipeline

pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('model', LogisticRegression())
])
pipe.fit(X_train, y_train)
```

> Reduces code repetition and error.

---

### ğŸŸ¦ Slide 20 â€“ Common Scikit-learn Models

| Task           | Model Classes                                         |
| -------------- | ----------------------------------------------------- |
| Classification | `LogisticRegression`, `RandomForestClassifier`, `SVC` |
| Regression     | `LinearRegression`, `Ridge`, `SVR`                    |
| Clustering     | `KMeans`, `DBSCAN`                                    |

---

### ğŸŸ¦ Slide 21 â€“ Case Study 1: Titanic Dataset

* **Goal**: Predict survival (`Survived`)
* Features: `Sex`, `Age`, `Pclass`, `Fare`, etc.
* Binary classification
* Example model: Logistic Regression

> Discuss preprocessing: filling missing ages, encoding gender, etc.

---

### ğŸŸ¦ Slide 22 â€“ Case Study 2: Boston Housing

* **Goal**: Predict median house price (`MEDV`)
* Regression task
* Input features: `CRIM`, `ZN`, `RM`, `LSTAT`, etc.
* Metrics: MSE, RÂ²

---

### ğŸŸ¦ Slide 23 â€“ Summary

âœ… Machine learning learns from data
âœ… Supervised vs. Unsupervised learning
âœ… Train/Test splits and overfitting
âœ… Scikit-learn provides modular tools
âœ… Pipelines standardize preprocessing and modeling

---

### ğŸŸ¦ Slide 24 â€“ Questions for Discussion

* When would you prefer unsupervised learning?
* What happens if we evaluate on training data?
* How does Scikit-learn simplify ML workflows?

---

### ğŸŸ¦ Slide 25 â€“ Teaser: Next Steps

â¡ï¸ **Next Lecture**: Model Evaluation
â¡ï¸ **Hands-on Lab**: Build your first ML model with Scikit-learn on the Titanic dataset
