
# ğŸŒ€ Apache Airflow Docker Lab

Welcome to the **Airflow Docker Lab**! This lab will help you learn how to:
- Install and run Apache Airflow using Docker
- Access the Airflow web interface
- Write and schedule your first DAGs (Directed Acyclic Graphs)
- Manage task execution and branching
- Build a simple real-world ETL pipeline

---

## Prerequisites

Make sure you have the following installed on your machine:

- [Docker](https://www.docker.com/products/docker-desktop/)
- [Docker Compose](https://docs.docker.com/compose/)
- A UNIX shell (Linux, macOS, or WSL on Windows)

---

## Step 1: Setup the Lab

### ğŸ—ï¸ 1. Clone or Download the Repository

You are inside the Airflow repository, use the Makefile for setup. Pulls the Apache Airflow Docker image:

```bash
make get-airflow
````

---

### ğŸ§ª 2. Initialize the Airflow Environment

```bash
make init
```

**What this does:**

* Creates folders: `dags/`, `logs/`, `plugins/`, `data/`
* Generates `.env` with your user ID
* Initializes the Airflow metadata database

---

### ğŸš€ 3. Start Airflow Services

```bash
make up
```

Wait for logs to stabilize and confirm that the **scheduler** and **webserver** are running.

---

## ğŸŒ Step 2: Access the Airflow UI

Open your browser and navigate to:

ğŸ‘‰ [http://localhost:8080](http://localhost:8080)

Get Credentials:
```bash
make get-credentials
```

Login using logged username and password:
```json
{"admin": "fdm9NbWagZMnzGnb"}
```


ğŸ” **Explore the UI:**

* DAGs list
* Graph View & Tree View
* Task Instance logs
* Admin panel (Connections, Variables)

---

## ğŸ§ª Step 3: Run Your First DAGs

All DAGs are stored in the `dags/` folder and will automatically appear in the UI.

### 1ï¸âƒ£ `hello_world.py`

* âœ… Purpose: Introduce a basic Python task
* ğŸ“ Action: Trigger manually and inspect logs
* ğŸ’¡ Takeaway: Understand basic DAG and task structure

```python
dag_id = "hello_world"
```

---

### 2ï¸âƒ£ `multi_step_pipeline.py`

* âœ… Purpose: Simulate a simple ETL pipeline
* ğŸ“ Action: Trigger DAG, explore task dependencies in Graph View
* ğŸ’¡ Takeaway: Learn how to chain tasks using `>>`

```python
extract >> transform >> load
```

---

### 3ï¸âƒ£ `conditional_pipeline.py`

* âœ… Purpose: Demonstrate branching logic using `BranchPythonOperator`
* ğŸ“ Action: Trigger multiple runs to observe random paths
* ğŸ’¡ Takeaway: Understand how dynamic task selection works

```python
choose_path() -> path_a or path_b -> join
```

---

### 4ï¸âƒ£ `real_pipeline.py`

* âœ… Purpose: Execute a mini real-world data pipeline
* ğŸ“ Action: Downloads NYC taxi data â†’ Cleans it with Pandas
* ğŸ’¡ Takeaway: Apply Python scripts in DAGs, use external packages

```python
download_csv >> clean_csv
```

---

## ğŸ§¹ Step 4: Clean Up

When youâ€™re done:

```bash
make down
```

To clean temporary files and logs:

```bash
make clean
```

---

## ğŸ“ Summary: What Youâ€™ve Learned

| Concept                    | Demonstrated In                   |
| -------------------------- | --------------------------------- |
| DAG structure              | `hello_world.py`                  |
| Task dependencies          | `multi_step_pipeline.py`          |
| Conditional branching      | `conditional_pipeline.py`         |
| Real-world ETL integration | `real_pipeline.py`                |
| Web UI monitoring          | All DAGs                          |
| Docker-based setup         | `Makefile`, `docker-compose.yaml` |

---

## ğŸ’¡ Next Steps (Optional Challenges)

* Add a task that sends an email on completion
* Use `FileSensor` to wait for a file to appear before running
* Modify `real_pipeline.py` to upload cleaned data to a database

